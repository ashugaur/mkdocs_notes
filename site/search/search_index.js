const local_index = {"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"mkdocs notes # Shared notes for easy read.","title":"mkdocs notes"},{"location":"index.html#mkdocs-notes","text":"Shared notes for easy read.","title":"mkdocs notes"},{"location":"ROC_AUC.html","text":"ROC & AUC # Bookmarks Reading reference Understanding confusion matrix Terms Term Description ROC Receiver operating characteristic Tells how good your model is compared to random chance >% good AUC Area under the ROC curve TP True positives TN True negatives FP False positives, Type I error FN False negatives, Type II error Confusion Matrix Confusion Matrix is cross-tabulation of actual vs predicted values It is used to see how well the model has performed Accuracy \\[Accuracy = \\frac{TP+TN}{Total}\\] Overall, how often is the classifier correct. Precision \\[Precision = \\frac{TP}{TP + FP}\\] When model predicts yes, how often is it correct? Precision should be as high as possible. A not precise model may find a lot of the positives, but its selection method is noisy. \u2003It also wrongly detects many positives that aren\u2019t actually positives. A precise model is very pure maybe it does not find all the positives. \u2003But the ones that the model does class as positive are very likely to be correct. Recall | Sensitivity | True Positive Rate \\[ Recall = \\frac{TP} {TP + FN}\\] Recall should be as high as possible. True positive rate measures the proportion of positives that are correctly identified as such, \u2003E.g. the percentage of sick people who are correctly identified as having the condition. Sensitivity refers to the test's ability to correctly detect patients who do have the condition. A model with high recall succeeds well in finding all the positive cases in the data, \u2003even though they may also wrongly identify some negative cases as positive cases. A model with low recall is not able to find all (or a large part) of the positive cases in the data. F1 score \\[F1 \\ score = 2 * \\frac{Precision \\ * Recall}{Precision \\ + Recall}\\] The F1 score is defined as the harmonic mean of precision and recall. F1 score gives equal weight to Precision and Recall. A model will obtain a high or low F1 score if both Precision and Recall are high or low. A model will obtain a medium F1 score if one of Precision and Recall is low and the other is high. Specificity | True Negative Rate \\[Specificity = \\frac{TN} {TN + FP}\\] True negative rate measures the proportion of negatives that are correctly identified as such, \u2003E.g., the percentage of healthy people who are correctly identified as not having the condition. Specificity relates to the test's ability to correctly detect patients without a condition. False Positive Rate \\[False \\ Positive \\ Rate = 1 - Specificity\\] Error Rate | Misclassification Rate \\[Misclassification \\ Rate = \\frac{FP+FN}{Total or 1 - Accuracy}\\] Misclassification Rate or Error Rate tells overall, how often model classification is wrong? Prevalence \\[Prevalence = \\frac{Actual \\ Yes}{Total}\\] How often does the yes condition actually occur in our sample. Above notes are taken most likely from bookmarks. \u21a9","title":"ROC AUC"},{"location":"ROC_AUC.html#roc-auc","text":"Bookmarks Reading reference Understanding confusion matrix Terms Term Description ROC Receiver operating characteristic Tells how good your model is compared to random chance >% good AUC Area under the ROC curve TP True positives TN True negatives FP False positives, Type I error FN False negatives, Type II error Confusion Matrix Confusion Matrix is cross-tabulation of actual vs predicted values It is used to see how well the model has performed Accuracy \\[Accuracy = \\frac{TP+TN}{Total}\\] Overall, how often is the classifier correct. Precision \\[Precision = \\frac{TP}{TP + FP}\\] When model predicts yes, how often is it correct? Precision should be as high as possible. A not precise model may find a lot of the positives, but its selection method is noisy. \u2003It also wrongly detects many positives that aren\u2019t actually positives. A precise model is very pure maybe it does not find all the positives. \u2003But the ones that the model does class as positive are very likely to be correct. Recall | Sensitivity | True Positive Rate \\[ Recall = \\frac{TP} {TP + FN}\\] Recall should be as high as possible. True positive rate measures the proportion of positives that are correctly identified as such, \u2003E.g. the percentage of sick people who are correctly identified as having the condition. Sensitivity refers to the test's ability to correctly detect patients who do have the condition. A model with high recall succeeds well in finding all the positive cases in the data, \u2003even though they may also wrongly identify some negative cases as positive cases. A model with low recall is not able to find all (or a large part) of the positive cases in the data. F1 score \\[F1 \\ score = 2 * \\frac{Precision \\ * Recall}{Precision \\ + Recall}\\] The F1 score is defined as the harmonic mean of precision and recall. F1 score gives equal weight to Precision and Recall. A model will obtain a high or low F1 score if both Precision and Recall are high or low. A model will obtain a medium F1 score if one of Precision and Recall is low and the other is high. Specificity | True Negative Rate \\[Specificity = \\frac{TN} {TN + FP}\\] True negative rate measures the proportion of negatives that are correctly identified as such, \u2003E.g., the percentage of healthy people who are correctly identified as not having the condition. Specificity relates to the test's ability to correctly detect patients without a condition. False Positive Rate \\[False \\ Positive \\ Rate = 1 - Specificity\\] Error Rate | Misclassification Rate \\[Misclassification \\ Rate = \\frac{FP+FN}{Total or 1 - Accuracy}\\] Misclassification Rate or Error Rate tells overall, how often model classification is wrong? Prevalence \\[Prevalence = \\frac{Actual \\ Yes}{Total}\\] How often does the yes condition actually occur in our sample. Above notes are taken most likely from bookmarks. \u21a9","title":"ROC &amp; AUC"}]}; var __search = { index: Promise.resolve(local_index) }